{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4CpjB6w3S1q"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=Warning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "VgEdtuf_4vLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to ndvi folder\n",
        "%cd drive/MyDrive/ndvi"
      ],
      "metadata": {
        "id": "H-V1yiCD4vmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the data files in the folder into a list and print it out\n",
        "ndvi_files_list = []\n",
        "for file in os.listdir():\n",
        "  if file.endswith('.csv'):\n",
        "    ndvi_files_list.append(file)\n",
        "\n",
        "print(\"List of Files in NDVI Folder:\")\n",
        "print(*ndvi_files_list, sep = \"\\n\")"
      ],
      "metadata": {
        "id": "Oy_EpYUU4vs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the combined ndvi data file\n",
        "ndvi_df = pd.read_csv(\"NDVI_Complete.csv\")\n",
        "display(ndvi_df)"
      ],
      "metadata": {
        "id": "YuzUYGRB4v1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current Directory:\", current_directory)"
      ],
      "metadata": {
        "id": "OvsL3kkx5Qc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to parent directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "GgRe711D5Xyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to gnvi folder\n",
        "%cd gndvi"
      ],
      "metadata": {
        "id": "iMNFEG3hXkj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the data files in the folder into a list and print it out\n",
        "gndvi_files_list = []\n",
        "for file in os.listdir():\n",
        "  if file.endswith('.csv'):\n",
        "    gndvi_files_list.append(file)\n",
        "\n",
        "print(\"List of Files in GNDVI Folder:\")\n",
        "print(*gndvi_files_list, sep = \"\\n\")"
      ],
      "metadata": {
        "id": "uaax1VmXXkmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the combined gndvi data file\n",
        "gndvi_df = pd.read_csv(\"GNDVI_Complete.csv\")\n",
        "display(gndvi_df)"
      ],
      "metadata": {
        "id": "C_xDNJMnXkpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current Directory:\", current_directory)"
      ],
      "metadata": {
        "id": "Jo00r6d6X2Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to parent directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "0vbyJeFtX5rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to no2 folder\n",
        "%cd no2"
      ],
      "metadata": {
        "id": "pkYYcVPt5Z9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the data files in the folder into a list and print it out\n",
        "no2_files_list = []\n",
        "for file in os.listdir():\n",
        "  if file.endswith('.csv'):\n",
        "    no2_files_list.append(file)\n",
        "\n",
        "print(\"List of Files in NO2 Folder:\")\n",
        "print(*no2_files_list, sep = \"\\n\")"
      ],
      "metadata": {
        "id": "QD7skr9T5er7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the combined no2 data file\n",
        "no2_df = pd.read_csv(\"NO2_Complete.csv\")\n",
        "display(no2_df)"
      ],
      "metadata": {
        "id": "jn2wBkic5jlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current Directory:\", current_directory)"
      ],
      "metadata": {
        "id": "fJFfOjsZ5p6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to parent directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "3D2b7zsu5t4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to soil moisture folder\n",
        "%cd soil_moisture"
      ],
      "metadata": {
        "id": "y68-lZs95wJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the data files into a list and print it out\n",
        "soil_moisture_files_list = []\n",
        "for file in os.listdir():\n",
        "  if file.endswith('.csv'):\n",
        "    soil_moisture_files_list.append(file)\n",
        "\n",
        "print(\"List of Files in Soil Moisture Folder:\")\n",
        "print(*soil_moisture_files_list, sep = \"\\n\")"
      ],
      "metadata": {
        "id": "z6r5e7a05zPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the combined soil moisture data file\n",
        "soil_moisture_df = pd.read_csv(\"Soil_Moisture_Complete.csv\")\n",
        "display(soil_moisture_df)"
      ],
      "metadata": {
        "id": "HhR4uRZS53Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current Directory:\", current_directory)"
      ],
      "metadata": {
        "id": "1DJzuB7j56GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to parent directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "odgxZHqY58qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to soil temperature folder\n",
        "%cd soil_temp"
      ],
      "metadata": {
        "id": "D5ELdDiR5_D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the data files in the folder into a list and print it out\n",
        "soil_temp_files_list = []\n",
        "for file in os.listdir():\n",
        "  if file.endswith('.csv'):\n",
        "    soil_temp_files_list.append(file)\n",
        "\n",
        "print(\"List of Files in Soil Temperature Folder:\")\n",
        "print(*soil_temp_files_list, sep = \"\\n\")"
      ],
      "metadata": {
        "id": "CVe1R4sz-fgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the combined soil temperature data file\n",
        "soil_temp_df = pd.read_csv(\"Soil_Temp_Complete.csv\")\n",
        "display(soil_temp_df)"
      ],
      "metadata": {
        "id": "ZXfS0eg5-lOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the date column in gndvi dataframe\n",
        "gndvi_df = gndvi_df.drop(columns=['date'])\n",
        "\n",
        "# Merge ndvi and gdnvi dataframes together\n",
        "ndvi_gndvi_df = pd.concat([ndvi_df, gndvi_df], axis=1)\n",
        "\n",
        "# Display the dataframe after merging\n",
        "display(ndvi_gndvi_df)"
      ],
      "metadata": {
        "id": "KinbUygp-pVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any rows where there is missing values in 'no2' column\n",
        "no2_filled_df = no2_df.copy()\n",
        "\n",
        "# Perform the interpolation on the copied dataframe\n",
        "no2_filled_df['no2'] = no2_filled_df['no2'].interpolate(method=\"linear\")\n",
        "\n",
        "# Print number of duplicated rows based the 'date' column\n",
        "no2_duplicatedRows_sum = no2_filled_df.duplicated(['date']).sum()\n",
        "print(\"Number of duplicated rows in NO2 dataframe:\", no2_duplicatedRows_sum)"
      ],
      "metadata": {
        "id": "p0Xc8_GwpUtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicated rows based the 'date' column\n",
        "no2_filled_df = no2_filled_df.drop_duplicates(subset=['date'], keep='last')\n",
        "\n",
        "# Print number of duplicated rows based the 'date' column after duplicates are removed\n",
        "no2_duplicatedRows_sum = no2_filled_df.duplicated(['date']).sum()\n",
        "print(\"Number of duplicated rows in NO2 dataframe:\", no2_duplicatedRows_sum)"
      ],
      "metadata": {
        "id": "Kdhb5O_irmIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'date' column to datetime object\n",
        "ndvi_gndvi_df['date'] = pd.to_datetime(ndvi_gndvi_df['date'])\n",
        "no2_filled_df['date'] = pd.to_datetime(no2_filled_df['date'])\n",
        "\n",
        "# Merge the ndvi_gndvi with no2 dataframes based on the 'date' column\n",
        "ndvi_gndvi_no2_df = pd.merge(ndvi_gndvi_df, no2_filled_df, on='date', how='inner')\n",
        "\n",
        "# Display the merged dataframe\n",
        "display(ndvi_gndvi_no2_df)"
      ],
      "metadata": {
        "id": "OsTuN2pakGxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any rows where there is missing values in 'soil_moisture' column\n",
        "soil_moisture_df = soil_moisture_df.dropna(subset=['soil_moisture'])\n",
        "\n",
        "# Print number of duplicated rows based the 'date' column\n",
        "soil_moisture_duplicatedRows_sum = soil_moisture_df.duplicated(['date']).sum()\n",
        "print(\"Number of duplicated rows in soil moisture dataframe:\", soil_moisture_duplicatedRows_sum)"
      ],
      "metadata": {
        "id": "LPygTfAgrlEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'date' column to datetime object\n",
        "soil_moisture_df['date'] = pd.to_datetime(soil_moisture_df['date'])\n",
        "\n",
        "# Merge ndvi_gndvi_no2 with soil_moisture dataframes based on the 'date' column\n",
        "ndvi_gndvi_no2_soilMoisture_df = pd.merge(ndvi_gndvi_no2_df, soil_moisture_df, on='date', how='inner')\n",
        "\n",
        "# Display the merged dataframe\n",
        "display(ndvi_gndvi_no2_soilMoisture_df)"
      ],
      "metadata": {
        "id": "6X7s2KZKBV0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any rows where there is missing values in 'soil_temp' column\n",
        "soil_temp_df = soil_temp_df.dropna(subset=['soil_temp'])\n",
        "\n",
        "# Print number of duplicated rows based the 'date' column\n",
        "soil_temp_duplicatedRows_sum = soil_temp_df.duplicated(['date']).sum()\n",
        "print(\"Number of duplicated rows in soil temperature dataframe:\", soil_temp_duplicatedRows_sum)"
      ],
      "metadata": {
        "id": "yb4wQNFCt7iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'date' column to datetime object\n",
        "soil_temp_df['date'] = pd.to_datetime(soil_temp_df['date'])\n",
        "\n",
        "# Merge ndvi_gndvi_no2_soilMoisture with soil_temp dataframes based on the 'date' column\n",
        "merged_df = pd.merge(ndvi_gndvi_no2_soilMoisture_df, soil_temp_df, on='date', how='inner')\n",
        "\n",
        "# Display the merged dataframe\n",
        "display(merged_df)\n",
        "\n",
        "merged_df.to_csv(\"merged.csv\")"
      ],
      "metadata": {
        "id": "-Hie-U-SDykv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicated rows based on the 'date' column\n",
        "merged_dropped_df = merged_df.drop_duplicates('date', keep='last')\n",
        "display(merged_dropped_df)"
      ],
      "metadata": {
        "id": "vidyvc88Eldp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any null values\n",
        "merged_dropped_df.isnull().sum()"
      ],
      "metadata": {
        "id": "9bbdN6EYFfmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 'date' column as the index\n",
        "data_df = merged_dropped_df.set_index('date')\n",
        "\n",
        "# Create new columns of 'day', 'month', and 'year'\n",
        "data_df['day'] = data_df.index.day\n",
        "data_df['month'] = data_df.index.month\n",
        "data_df['year'] = data_df.index.year\n",
        "display(data_df)"
      ],
      "metadata": {
        "id": "rFOWPXn6GGoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new column 'growing_season' based on the time period\n",
        "data_df['growing_season'] = np.where(\n",
        "    (data_df['month'] >= 3) & (data_df['month'] <= 5),\n",
        "    1,  # March to May\n",
        "    np.where(\n",
        "        (data_df['month'] >= 9) & (data_df['month'] <= 11),\n",
        "        2,  # September to November\n",
        "        np.nan\n",
        "    )\n",
        ")\n",
        "\n",
        "data_df['growing_season'] = data_df['growing_season'].astype('Int64')\n",
        "\n",
        "# Display the dataframe\n",
        "display(data_df)"
      ],
      "metadata": {
        "id": "c-PSWtZ9HUCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add yield value to each group\n",
        "def add_yield_value(group):\n",
        "    if group['year'].iloc[0] == 2019 and group['growing_season'].iloc[0] == 1:\n",
        "        yield_value = 20686.4\n",
        "    elif group['year'].iloc[0] == 2019 and group['growing_season'].iloc[0] == 2:\n",
        "        yield_value = 19324.8\n",
        "    elif group['year'].iloc[0] == 2020 and group['growing_season'].iloc[0] == 1:\n",
        "        yield_value = 19985.7\n",
        "    elif group['year'].iloc[0] == 2020 and group['growing_season'].iloc[0] == 2:\n",
        "        yield_value = 21245.3\n",
        "    elif group['year'].iloc[0] == 2021 and group['growing_season'].iloc[0] == 1:\n",
        "        yield_value = 17222.4\n",
        "    elif group['year'].iloc[0] == 2021 and group['growing_season'].iloc[0] == 2:\n",
        "        yield_value = 19249.4\n",
        "\n",
        "    # Initialise the 'yield' column with value 0\n",
        "    group['yield'] = 0\n",
        "\n",
        "    # Assign the yield value to the last row of its corresponding group\n",
        "    group['yield'].iloc[-1] = yield_value\n",
        "\n",
        "    return group\n",
        "\n",
        "# Apply function to add the yield value to the dataframe\n",
        "complete_data_df = data_df.groupby(['year', 'growing_season'], group_keys=False).apply(add_yield_value)\n",
        "\n",
        "# Display the dataframe\n",
        "display(complete_data_df)"
      ],
      "metadata": {
        "id": "qp2h5mdlY5Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get current directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current Directory:\", current_directory)"
      ],
      "metadata": {
        "id": "F9Op1kmafDyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to parent directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "XNIYekmRfGke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export dataframe to csv in Google Drive\n",
        "complete_data_df.to_csv(\"Data.csv\")"
      ],
      "metadata": {
        "id": "_mwSuF6vcGqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data csv file as dataframe\n",
        "data = pd.read_csv(\"Data.csv\")\n",
        "display(data)"
      ],
      "metadata": {
        "id": "nAgP--1wgxzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the minimum number of rows out of all the groups\n",
        "min_row_count = data.groupby(['year', 'growing_season']).size().min()\n",
        "print(\"Period:\", min_row_count)"
      ],
      "metadata": {
        "id": "QKnPLx2ghNaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove additional rows to ensure each group has same number of rows by keeping the tail of each group\n",
        "data = data.groupby(['year', 'growing_season']).tail(min_row_count)\n",
        "display(data)"
      ],
      "metadata": {
        "id": "PmwGGBnWhnBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display the observed values for each feature\n",
        "def decompose_observed(df, features, period):\n",
        "    # Create subplots for each feature\n",
        "    fig, axis = plt.subplots(len(features), figsize=(16,10))\n",
        "\n",
        "    # Adjust the vertical space between each subplots\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    # Add title\n",
        "    fig.suptitle('Observed Values', fontsize=20)\n",
        "\n",
        "    # Loop to plot the observed value for each feature\n",
        "    for i, feature in enumerate(features):\n",
        "        # Decomposing time series value into time series components\n",
        "        decomposed = seasonal_decompose(df[feature].values, period=period)\n",
        "\n",
        "        # Get the component of observed\n",
        "        observed = decomposed.observed\n",
        "\n",
        "        # Plot the observed values for each feature\n",
        "        axis[i].set_title(f'Observed ({feature})', fontsize=16)\n",
        "        axis[i].plot(observed)\n",
        "        axis[i].grid()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "# Function to display the trend of each feature\n",
        "def decompose_trend(df, features, period):\n",
        "    # Create subplots for each feature\n",
        "    fig, axis = plt.subplots(len(features), figsize=(16,10))\n",
        "\n",
        "    # Adjust the vertical space between subplots\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    # Add title\n",
        "    fig.suptitle('Trend', fontsize=20)\n",
        "\n",
        "    # Loop to plot the trend for each feature\n",
        "    for i, feature in enumerate(features):\n",
        "        # Decomposing time series value into time series components\n",
        "        decomposed = seasonal_decompose(df[feature].values, period=period)\n",
        "\n",
        "        # Get the component of trend\n",
        "        trend = decomposed.trend\n",
        "\n",
        "        # Plot the trend of each feature\n",
        "        axis[i].set_title(f'Trend ({feature})', fontsize=16)\n",
        "        axis[i].plot(trend)\n",
        "        axis[i].grid()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "# Function to display the seasonality of each feature\n",
        "def decompose_seasonality(df, features, period):\n",
        "    # Create subplots for each feature\n",
        "    fig, axis = plt.subplots(len(features), figsize=(16,10))\n",
        "\n",
        "    # Adjust the vertical space between subplots\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    # Add title\n",
        "    fig.suptitle('Seasonality', fontsize=20)\n",
        "\n",
        "    # Loop to plot the seasonality for each feature\n",
        "    for i, feature in enumerate(features):\n",
        "        # Decomposing time series value into time series components\n",
        "        decomposed = seasonal_decompose(df[feature].values, period=period)\n",
        "\n",
        "        # Get the component of seasonality\n",
        "        seasonality = decomposed.seasonal\n",
        "\n",
        "        # Plot the seasonality of each feature\n",
        "        axis[i].set_title(f'Seasonality ({feature})', fontsize=16)\n",
        "        axis[i].plot(seasonality)\n",
        "        axis[i].grid()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "# Function to display the residual of each feature\n",
        "def decompose_residual(df, features, period):\n",
        "    # Create subplots for each feature\n",
        "    fig, axis = plt.subplots(len(features), figsize=(16,10))\n",
        "\n",
        "    # Adjust the vertical space between subplots\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    # Add title\n",
        "    fig.suptitle('Residual', fontsize=20)\n",
        "\n",
        "    # Loop to plot the residual for each feature\n",
        "    for i, feature in enumerate(features):\n",
        "        # Decomposing time series value into time series components\n",
        "        decomposed = seasonal_decompose(df[feature].values, period=period)\n",
        "\n",
        "        # Get the component of residual\n",
        "        residual = decomposed.resid\n",
        "\n",
        "        # Plot the residual of each feature\n",
        "        axis[i].set_title(f'Residual ({feature})', fontsize=16)\n",
        "        axis[i].plot(residual)\n",
        "        axis[i].grid()\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NfwCl4EOiOG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call decompose_observed function\n",
        "decompose_observed(data, ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp'], 14)"
      ],
      "metadata": {
        "id": "CK7Rm2Ivj6zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call decompose_trend function\n",
        "decompose_trend(data, ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp'], 14)"
      ],
      "metadata": {
        "id": "sJmPFWulzYYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call decompose_seasonality function\n",
        "decompose_seasonality(data, ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp'], 14)"
      ],
      "metadata": {
        "id": "0Dn0-IDFAUwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call decompose_residual function\n",
        "decompose_residual(data, ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp'], 14)"
      ],
      "metadata": {
        "id": "55iW9s15AU48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to identify and treat the outliers\n",
        "def identify_and_treatment_outliers(df, features, period):\n",
        "    # Decomposing time series value into time series components\n",
        "    res = seasonal_decompose(df[features], period=period)\n",
        "\n",
        "    # Get the component of residual\n",
        "    residual = res.resid\n",
        "\n",
        "    # Calculate the first quartile, second quartile, and IQR\n",
        "    q1 = residual.quantile(0.25)\n",
        "    q3 = residual.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # Identify the outliers\n",
        "    outliers = (residual > (q3 + (iqr * 1.5))) | (residual < (q1 - (iqr * 1.5)))\n",
        "\n",
        "    # Replace outliers values with linear interpolated values\n",
        "    residual[outliers] = residual.interpolate(method=\"linear\")"
      ],
      "metadata": {
        "id": "8jm3xRt1FenI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treat outliers by calling the identify_and_treatment_outliers function\n",
        "identify_and_treatment_outliers(data, 'ndvi', 14)\n",
        "identify_and_treatment_outliers(data, 'gndvi', 14)\n",
        "identify_and_treatment_outliers(data, 'no2', 14)\n",
        "identify_and_treatment_outliers(data, 'soil_moisture', 14)\n",
        "identify_and_treatment_outliers(data, 'soil_temp', 14)"
      ],
      "metadata": {
        "id": "uCt18dWpFqke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features for which normalisations will be performed on\n",
        "features_normalise = ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp', 'yield']\n",
        "\n",
        "# Normalize the features using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "data[features_normalise] = scaler.fit_transform(data[features_normalise])\n",
        "\n",
        "# Display the dataframe after normalisation\n",
        "display(data)"
      ],
      "metadata": {
        "id": "7z4egQsW2n0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the current directory\n",
        "current_directory = os.getcwd()\n",
        "print(\"Current Directory:\", current_directory)"
      ],
      "metadata": {
        "id": "qfa2wwaEQa2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the dataframe into csv file\n",
        "data.to_csv(\"Data_Training.csv\")"
      ],
      "metadata": {
        "id": "c2Qx4zJdQkBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regression\n",
        "\n",
        "# Read the data csv file into a dataframe\n",
        "rf_data_df = pd.read_csv(\"Data_Training.csv\")\n",
        "\n",
        "# Features where lagging will be done\n",
        "rf_features_to_lag = ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp']\n",
        "\n",
        "# Specify the lag days\n",
        "rf_lag_days = 13\n",
        "\n",
        "# Create lagged columns within each group defined by 'year' and 'growing_season'\n",
        "def lag_days_range(x):\n",
        "    return range(1, len(x) + 1)\n",
        "\n",
        "# Create lagged columns for each feature in 'features_to_lag'\n",
        "for feature in rf_features_to_lag:\n",
        "    # Create '{column}_lag_days' column\n",
        "    rf_data_df[f'{feature}_lag_days'] = rf_data_df.groupby(['year', 'growing_season'])[feature].transform(lag_days_range)\n",
        "\n",
        "    # Create lagged columns for each day\n",
        "    for i in range(1, rf_lag_days + 1):\n",
        "        rf_data_df[f'{feature}_lagged_{i}'] = rf_data_df.groupby(['year', 'growing_season'])[feature].shift(i)\n",
        "\n",
        "# Convert lagged columns to float\n",
        "rf_lagged_columns = [f'{feature}_lagged_{lag}' for feature in rf_features_to_lag for lag in range(1, rf_lag_days + 1)]\n",
        "rf_data_df[rf_lagged_columns] = rf_data_df[rf_lagged_columns].astype(float)\n",
        "\n",
        "# Drop rows with missing values\n",
        "rf_data_df = rf_data_df.dropna()\n",
        "\n",
        "# Split the data into training and testing sets based on the year\n",
        "rf_unique_year_count = len(rf_data_df['year'].unique())\n",
        "rf_train_years = int(rf_unique_year_count * 0.8)\n",
        "rf_split_index = rf_data_df['year'].unique()[rf_train_years - 1]\n",
        "rf_train = rf_data_df[rf_data_df['year'] <= rf_split_index].dropna(subset=['yield'])\n",
        "rf_test = rf_data_df[rf_data_df['year'] > rf_split_index].dropna(subset=['yield'])\n",
        "\n",
        "# Define the features and target variable\n",
        "rf_features = rf_lagged_columns + ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp', 'day','month','year','growing_season']\n",
        "rf_target = 'yield'\n",
        "\n",
        "# Drop rows with missing values\n",
        "rf_data_df = rf_data_df.dropna(subset=rf_features)\n",
        "\n",
        "# Define the train and test data\n",
        "rf_X_train, rf_y_train = rf_train[rf_features], rf_train[rf_target]\n",
        "rf_X_test, rf_y_test = rf_test[rf_features], rf_test[rf_target]\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(rf_X_train, rf_y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "rf_y_pred = rf_model.predict(rf_X_test)\n",
        "\n",
        "# Evaluate the RF model performance\n",
        "rf_mse = mean_squared_error(rf_y_test, rf_y_pred)\n",
        "rf_mse_perHectare = (rf_mse/4435.343)*1000\n",
        "\n",
        "rf_rmse = np.sqrt(mean_squared_error(rf_y_test, rf_y_pred))\n",
        "rf_rmse_perHectare = (rf_rmse/4435.343)*1000\n",
        "\n",
        "rf_mae = mean_absolute_error(rf_y_test, rf_y_pred)\n",
        "rf_mae_perHectare = (rf_mae/4435.343)*1000\n",
        "\n",
        "print('Model Performance of Random Forest\\n')\n",
        "print(f'Mean Squared Error (MSE): {rf_mse_perHectare:.6f} kg/ha')\n",
        "print(f'Root Mean Squared Error (RMSE): {rf_rmse_perHectare:.6f} kg/ha')\n",
        "print(f'Mean Absolute Error (MAE): {rf_mae_perHectare:.6f} kg/ha')"
      ],
      "metadata": {
        "id": "L6FNZ7Je_4CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM for Growing Season 1\n",
        "\n",
        "# Set random seeds\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "\n",
        "# Read the data csv file into a dataframe\n",
        "lstm_gs1_data_df = pd.read_csv(\"Data_Training.csv\")\n",
        "\n",
        "# Change 'year' to a categorical variable\n",
        "lstm_gs1_data_df['year'] = lstm_gs1_data_df['year'].astype('category')\n",
        "\n",
        "# Define the number of time steps\n",
        "time_steps_lstm_gs1 = 14\n",
        "unique_years_lstm_gs1 = lstm_gs1_data_df['year'].unique()\n",
        "\n",
        "# Split the data into training and testing sets based on the year\n",
        "split_index_lstm_gs1 = int(0.8 * len(unique_years_lstm_gs1))\n",
        "train_years_lstm_gs1, test_years_lstm_gs1 = unique_years_lstm_gs1[:split_index_lstm_gs1], unique_years_lstm_gs1[split_index_lstm_gs1:]\n",
        "\n",
        "# Store the training data\n",
        "X_train_list_lstm_gs1 = []\n",
        "y_train_list_lstm_gs1 = []\n",
        "\n",
        "# Loop through training years to accumulate training data\n",
        "for year in train_years_lstm_gs1:\n",
        "    # Get the data for a group based on growing season and year\n",
        "    group_df = lstm_gs1_data_df[(lstm_gs1_data_df['growing_season'] == 1) & (lstm_gs1_data_df['year'] == year)].copy()\n",
        "\n",
        "    # Define the features and target variable\n",
        "    target_lstm_gs1 = 'yield'\n",
        "    features_lstm_gs1 = ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp']\n",
        "\n",
        "    # Create a new column 'year_feature'\n",
        "    group_df['year_feature'] = group_df['year'].cat.codes\n",
        "\n",
        "    # Get the features values and target variable value\n",
        "    X_lstm_gs1 = group_df[features_lstm_gs1 + ['year_feature']].values\n",
        "    y_lstm_gs1 = group_df[target_lstm_gs1].values[-1]\n",
        "\n",
        "    X_train_list_lstm_gs1.append(X_lstm_gs1)\n",
        "    y_train_list_lstm_gs1.append(y_lstm_gs1)\n",
        "\n",
        "# Combine the training data across the years\n",
        "X_train_lstm_gs1 = np.concatenate(X_train_list_lstm_gs1)\n",
        "y_train_lstm_gs1 = np.array(y_train_list_lstm_gs1)\n",
        "\n",
        "# Reshape the input data for LSTM\n",
        "X_train_lstm_gs1 = X_train_lstm_gs1.reshape((X_train_lstm_gs1.shape[0] // time_steps_lstm_gs1, time_steps_lstm_gs1, X_train_lstm_gs1.shape[1]))\n",
        "\n",
        "# Build LSTM Model\n",
        "model_lstm_gs1 = Sequential()\n",
        "model_lstm_gs1.add(LSTM(units=50, input_shape=(time_steps_lstm_gs1, X_train_lstm_gs1.shape[2])))\n",
        "model_lstm_gs1.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm_gs1.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Fit the model with the training data\n",
        "model_lstm_gs1.fit(X_train_lstm_gs1, y_train_lstm_gs1, epochs=100, batch_size=32)\n",
        "\n",
        "# Test the model on testing data\n",
        "for year in test_years_lstm_gs1:\n",
        "    # Get the data for a group based on growing season and year\n",
        "    group_df = lstm_gs1_data_df[(lstm_gs1_data_df['growing_season'] == 1) & (lstm_gs1_data_df['year'] == year)].copy()\n",
        "\n",
        "    # Define the features and target variable\n",
        "    target_lstm_gs1 = 'yield'\n",
        "    features_lstm_gs1 = ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp']\n",
        "\n",
        "    # Create a new column 'year_feature'\n",
        "    group_df['year_feature'] = group_df['year'].cat.codes\n",
        "\n",
        "    # Get the features values and target variable value\n",
        "    X_test_lstm_gs1 = group_df[features_lstm_gs1 + ['year_feature']].values\n",
        "    y_test_lstm_gs1 = group_df[target_lstm_gs1].values[-1]\n",
        "\n",
        "    # Reshape the input data for LSTM\n",
        "    X_test_lstm_gs1 = X_test_lstm_gs1.reshape((1, time_steps_lstm_gs1, X_test_lstm_gs1.shape[1]))\n",
        "\n",
        "    # Get the predictions of the model\n",
        "    predictions_lstm_gs1 = model_lstm_gs1.predict(X_test_lstm_gs1)\n",
        "\n",
        "    # Evaluate the LSTM model\n",
        "    lstm_gs1_mse = mean_squared_error(np.array([y_test_lstm_gs1]), predictions_lstm_gs1)\n",
        "    lstm_gs1_mse_perHectare = (lstm_gs1_mse/4435.343)*1000\n",
        "\n",
        "    lstm_gs1_rmse = np.sqrt(lstm_gs1_mse)\n",
        "    lstm_gs1_rmse_perHectare = (lstm_gs1_rmse/4435.343)*1000\n",
        "\n",
        "    lstm_gs1_mae = mean_absolute_error(np.array([y_test_lstm_gs1]), predictions_lstm_gs1)\n",
        "    lstm_gs1_mae_perHectare = (lstm_gs1_mae/4435.343)*1000\n",
        "\n",
        "    print('\\nModel Performance of LSTM Growing Season 1\\n')\n",
        "    print(f'Mean Squared Error (MSE): {lstm_gs1_mse_perHectare:.6f} kg/ha')\n",
        "    print(f'Root Mean Squared Error (RMSE): {lstm_gs1_rmse_perHectare:.6f} kg/ha')\n",
        "    print(f'Mean Absolute Error (MAE): {lstm_gs1_mae_perHectare:.6f} kg/ha')"
      ],
      "metadata": {
        "id": "5kFzyylt0gaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM for Growing Season 2\n",
        "\n",
        "# Set random seeds\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)\n",
        "random.seed(seed_value)\n",
        "\n",
        "# Read the data csv file into a dataframe\n",
        "lstm_gs2_data_df = pd.read_csv(\"Data_Training.csv\")\n",
        "\n",
        "# Change 'year' to a categorical variable\n",
        "lstm_gs2_data_df['year'] = lstm_gs2_data_df['year'].astype('category')\n",
        "\n",
        "# Define the number of time steps\n",
        "time_steps_lstm_gs2 = 14\n",
        "unique_years_lstm_gs2 = lstm_gs2_data_df['year'].unique()\n",
        "\n",
        "# Split the data into training and testing sets based on the year\n",
        "split_index_lstm_gs2 = int(0.8 * len(unique_years_lstm_gs2))\n",
        "train_years_lstm_gs2, test_years_lstm_gs2 = unique_years_lstm_gs2[:split_index_lstm_gs2], unique_years_lstm_gs2[split_index_lstm_gs2:]\n",
        "\n",
        "# Store the training data\n",
        "X_train_list_lstm_gs2 = []\n",
        "y_train_list_lstm_gs2 = []\n",
        "\n",
        "# Loop through training years to accumulate training data\n",
        "for year in train_years_lstm_gs2:\n",
        "    # Get the data for a group based on growing season and year\n",
        "    group_df = lstm_gs2_data_df[(lstm_gs2_data_df['growing_season'] == 2) & (lstm_gs2_data_df['year'] == year)].copy()\n",
        "\n",
        "    # Define the features and target variable\n",
        "    target_lstm_gs2 = 'yield'\n",
        "    features_lstm_gs2 = ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp']\n",
        "\n",
        "    # Create a new column 'year_feature'\n",
        "    group_df['year_feature'] = group_df['year'].cat.codes\n",
        "\n",
        "    # Get the features values and target variable value\n",
        "    X_lstm_gs2 = group_df[features_lstm_gs2 + ['year_feature']].values\n",
        "    y_lstm_gs2 = group_df[target_lstm_gs2].values[-1]\n",
        "\n",
        "    X_train_list_lstm_gs2.append(X_lstm_gs2)\n",
        "    y_train_list_lstm_gs2.append(y_lstm_gs2)\n",
        "\n",
        "# Combine the training data across the years\n",
        "X_train_lstm_gs2 = np.concatenate(X_train_list_lstm_gs2)\n",
        "y_train_lstm_gs2 = np.array(y_train_list_lstm_gs2)\n",
        "\n",
        "# Reshape the input data for LSTM\n",
        "X_train_lstm_gs2 = X_train_lstm_gs2.reshape((X_train_lstm_gs2.shape[0] // time_steps_lstm_gs2, time_steps_lstm_gs2, X_train_lstm_gs2.shape[1]))\n",
        "\n",
        "# Build LSTM Model\n",
        "model_lstm_gs2 = Sequential()\n",
        "model_lstm_gs2.add(LSTM(units=50, input_shape=(time_steps_lstm_gs2, X_train_lstm_gs2.shape[2])))\n",
        "model_lstm_gs2.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm_gs2.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Fit the model with the training data\n",
        "model_lstm_gs2.fit(X_train_lstm_gs2, y_train_lstm_gs2, epochs=100, batch_size=32)\n",
        "\n",
        "# Test the model on testing data\n",
        "for year in test_years_lstm_gs2:\n",
        "    # Get the data for a group based on growing season and year\n",
        "    group_df = lstm_gs2_data_df[(lstm_gs2_data_df['growing_season'] == 2) & (lstm_gs2_data_df['year'] == year)].copy()\n",
        "\n",
        "    # Define the features and target variable\n",
        "    target_lstm_gs2 = 'yield'\n",
        "    features_lstm_gs2 = ['ndvi', 'gndvi', 'no2', 'soil_moisture', 'soil_temp']\n",
        "\n",
        "    # Create a new column 'year_feature'\n",
        "    group_df['year_feature'] = group_df['year'].cat.codes\n",
        "\n",
        "    # Get the features values and target variable value\n",
        "    X_test_lstm_gs2 = group_df[features_lstm_gs2 + ['year_feature']].values\n",
        "    y_test_lstm_gs2 = group_df[target_lstm_gs2].values[-1]\n",
        "\n",
        "    # Reshape the input data for LSTM\n",
        "    X_test_lstm_gs2 = X_test_lstm_gs2.reshape((1, time_steps_lstm_gs2, X_test_lstm_gs2.shape[1]))\n",
        "\n",
        "    # Get the predictions of the model\n",
        "    predictions_lstm_gs2 = model_lstm_gs2.predict(X_test_lstm_gs2)\n",
        "\n",
        "    # Evaluate the LSTM model\n",
        "    lstm_gs2_mse = mean_squared_error(np.array([y_test_lstm_gs2]), predictions_lstm_gs2)\n",
        "    lstm_gs2_mse_perHectare = (lstm_gs2_mse/4435.343)*1000\n",
        "\n",
        "    lstm_gs2_rmse = np.sqrt(lstm_gs2_mse)\n",
        "    lstm_gs2_rmse_perHectare = (lstm_gs2_rmse/4435.343)*1000\n",
        "\n",
        "    lstm_gs2_mae = mean_absolute_error(np.array([y_test_lstm_gs2]), predictions_lstm_gs2)\n",
        "    lstm_gs2_mae_perHectare = (lstm_gs2_mae/4435.343)*1000\n",
        "\n",
        "    print('\\nModel Performance of LSTM Growing Season 2\\n')\n",
        "    print(f'Mean Squared Error (MSE): {lstm_gs2_mse_perHectare:.6f} kg/ha')\n",
        "    print(f'Root Mean Squared Error (RMSE): {lstm_gs2_rmse_perHectare:.6f} kg/ha')\n",
        "    print(f'Mean Absolute Error (MAE): {lstm_gs2_mae_perHectare:.6f} kg/ha')"
      ],
      "metadata": {
        "id": "QDa9eFmw53XR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}